import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import random
import asyncio
from typing import Optional

class HuggingFaceHandler:
    def __init__(self):
        print("=== HuggingFace ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ ===")
        self.model_name = "EleutherAI/polyglot-ko-5.8b"
        
        print("1. í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="left",
        )
        print("âœ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
        
        print("2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        torch.cuda.empty_cache()
        print("âœ“ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        print("3. ëª¨ë¸ ë¡œë”© ì¤‘... (1-3ë¶„ ì†Œìš”)")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        self.model_loaded = True
        print("=== ì´ˆê¸°í™” ì™„ë£Œ! ì„œë¹„ìŠ¤ ì¤€ë¹„ë¨ ===")
            
        self.model_loaded = True
        self.emojis = ['ğŸ’•', 'âœ¨', 'ğŸ¥º', 'ğŸ˜Š', 'ğŸ’', 'ğŸŒ¸', 'ğŸ’—', 'ğŸ’–']
        
        # ì¶”ë¡  íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ)
        self.inference_timeout = 30   # ì¶”ë¡  íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ì„¤ì •

    async def get_completion(self, message: str, style: str) -> Optional[str]:
        if not self.model_loaded:
            print("[ìƒíƒœ] ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
            
        try:
            if style == 'cute':
                prompt = f"""ë‹¤ìŒ ë¬¸ì¥ì„ ê·€ì—½ê³  ë°œë„í•œ ë§íˆ¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

                ê·œì¹™:
                1. "~ìš©", "~ì–", "~ëƒ¥" ê°™ì€ ê·€ì—¬ìš´ ì–´ë¯¸ ì‚¬ìš©í•˜ê¸°
                2. ë°ê³  ê¸ì •ì ì¸ í†¤ìœ¼ë¡œ ë³€í™˜í•˜ê¸°
                3. ì§§ê³  ê°„ë‹¨í•˜ê²Œ ë³€í™˜í•˜ê¸°
                4. ë¬¸ì¥ ëì—ëŠ” ëŠë‚Œí‘œë‚˜ ë¬¼ìŒí‘œ ì‚¬ìš©í•˜ê¸°

                ì…ë ¥: "{message}"
                ì¶œë ¥:"""
            else:
                prompt = f"""ë‹¤ìŒ ë¬¸ì¥ì„ ë³€í™˜í•´ì£¼ì„¸ìš”.
                ì…ë ¥: "{message}"
                ì¶œë ¥:"""

            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.model.device)

            # ì¶”ë¡  ì‹œê°„ë§Œ íƒ€ì„ì•„ì›ƒ ì ìš©
            print("[ì²˜ë¦¬] ì¶”ë¡  ì‹œì‘...")
            outputs = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_new_tokens=32,
                        temperature=0.7,
                        do_sample=True
                    )
                ),
                timeout=self.inference_timeout
            )
            print("[ì²˜ë¦¬] ì¶”ë¡  ì™„ë£Œ")

            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()

            if style == 'cute':
                emoji_count = random.randint(1, 2)
                selected_emojis = ' ' + ''.join(random.sample(self.emojis, emoji_count))
                response = response + selected_emojis

            return response

        except asyncio.TimeoutError:
            print(f"[íƒ€ì„ì•„ì›ƒ] {self.inference_timeout}ì´ˆ ì´ˆê³¼")
            return None
        except Exception as e:
            print(f"[ì—ëŸ¬] HuggingFace ëª¨ë¸ ì˜¤ë¥˜: {e}")
            return None