import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import random
import asyncio
from typing import Optional
import concurrent.futures

class HuggingFaceHandler:
    def __init__(self):
        print("=== HuggingFace ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ ===")
        self.model_name = "EleutherAI/polyglot-ko-5.8b"
        
        # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        print("1. í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="left",
        )
        print("âœ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
        
        print("2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
        
        print("3. ëª¨ë¸ ë¡œë”© ì¤‘... (1-3ë¶„ ì†Œìš”)")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,  # ë°˜ì •ë°€ë„(FP16) ì‚¬ìš©
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        )
        self.model.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        
        self.model_loaded = True
        print("=== ì´ˆê¸°í™” ì™„ë£Œ! ì„œë¹„ìŠ¤ ì¤€ë¹„ë¨ ===")
            
        self.model_loaded = True
        self.emojis = ['ğŸ’•', 'âœ¨', 'ğŸ¥º', 'ğŸ˜Š', 'ğŸ’', 'ğŸŒ¸', 'ğŸ’—', 'ğŸ’–']
        
        # ì¶”ë¡  íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ)
        self.inference_timeout = 300   # 60ì´ˆë¡œ ìˆ˜ì •

    async def get_completion(self, message: str, style: str) -> Optional[str]:
        if not self.model_loaded:
            print("[ìƒíƒœ] ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
            
        try:
            # ìŠ¤íƒ€ì¼ë³„ í”„ë¡¬í”„íŠ¸ ì •ì˜
            style_prompts = {
                'formal': f"""ì•„ë˜ ë¬¸ì¥ì„ ê²©ì‹ì²´ë¡œ ë°”ê¾¸ì–´ ì£¼ì„¸ìš”. ë¬¸ì¥ì˜ ì›ë˜ ì˜ë¯¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ë§íˆ¬ë§Œ ë³€ê²½í•˜ì„¸ìš”.
                ì›ë¬¸: "{message}"
                ë³€í™˜ëœ ë¬¸ì¥:""",
                
                'casual': f"""ì•„ë˜ ë¬¸ì¥ì„ ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ë§íˆ¬ë¡œ ë°”ê¾¸ì–´ ì£¼ì„¸ìš”. ë¬¸ì¥ì˜ ì›ë˜ ì˜ë¯¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ë§íˆ¬ë§Œ ë³€ê²½í•˜ì„¸ìš”.
                ì›ë¬¸: "{message}"
                ë³€í™˜ëœ ë¬¸ì¥:""",
                
                'polite': f"""ì•„ë˜ ë¬¸ì¥ì„ ê³µì†í•˜ê³  ì˜ˆì˜ ë°”ë¥¸ ë§íˆ¬ë¡œ ë°”ê¾¸ì–´ ì£¼ì„¸ìš”. ë¬¸ì¥ì˜ ì›ë˜ ì˜ë¯¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ë§íˆ¬ë§Œ ë³€ê²½í•˜ì„¸ìš”.
                ì›ë¬¸: "{message}"
                ë³€í™˜ëœ ë¬¸ì¥:""",
                
                'cute': f"""ì•„ë˜ ë¬¸ì¥ì„ ê·€ì—½ê³  ì• êµìˆëŠ” ë§íˆ¬ë¡œ ë°”ê¾¸ì–´ ì£¼ì„¸ìš”. ë¬¸ì¥ì˜ ì›ë˜ ì˜ë¯¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ ë§íˆ¬ë§Œ ë³€ê²½í•˜ì„¸ìš”.
                ì›ë¬¸: "{message}"
                ë³€í™˜ëœ ë¬¸ì¥:"""
            }

            prompt = style_prompts.get(style, style_prompts['casual'])  # ê¸°ë³¸ê°’ì€ ì¹œê·¼ì²´

            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)  # ëª…ì‹œì ìœ¼ë¡œ device ì§€ì •

            if 'token_type_ids' in inputs:
                del inputs['token_type_ids']

            outputs = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **inputs,
                        max_new_tokens=100,     
                        temperature=0.3,        # ë” ë‚®ì¶°ì„œ ì•ˆì •ì„± í™•ë³´
                        do_sample=True,
                        top_p=0.85,            # ë” ë‚®ì¶°ì„œ ê´€ë ¨ì„± ë†’ì€ í† í°ë§Œ ì„ íƒ
                        repetition_penalty=1.1,  # ì•½ê°„ì˜ ë°˜ë³µ ë°©ì§€ë§Œ ì ìš©
                        num_beams=3,           # ë¹” ì„œì¹˜ ì¶”ê°€
                        early_stopping=True     # ì ì ˆí•œ ì‹œì ì— ìƒì„± ì¤‘ë‹¨
                    )
                ),
                timeout=self.inference_timeout
            )

            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()

            # cute ìŠ¤íƒ€ì¼ì¼ ë•Œë§Œ ì´ëª¨ì§€ ì¶”ê°€
            if style == 'cute':
                emoji_count = random.randint(1, 2)
                selected_emojis = ' ' + ''.join(random.sample(self.emojis, emoji_count))
                response = response + selected_emojis

            return response

        except asyncio.TimeoutError:
            print(f"[íƒ€ì„ì•„ì›ƒ] {self.inference_timeout}ì´ˆ ì´ˆê³¼")
            return None
        except Exception as e:
            print(f"[ì—ëŸ¬] HuggingFace ëª¨ë¸ ì˜¤ë¥˜: {e}")
            return None
    def _generate_response(self, message: str, style: str) -> str:
        # ê¸°ì¡´ì˜ ë™ê¸° ì²˜ë¦¬ ì½”ë“œë¥¼ ì—¬ê¸°ë¡œ ì´ë™
        if style == 'cute':
            prompt = f"""ë‹¤ìŒ ë¬¸ì¥ì„ ê·€ì—½ê³  ë°œë„í•œ ë§íˆ¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”...."""
        else:
            prompt = f"""ë‹¤ìŒ ë¬¸ì¥ì„ ë³€í™˜í•´ì£¼ì„¸ìš”...."""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**inputs)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return response
