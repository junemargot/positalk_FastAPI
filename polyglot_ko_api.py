import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import random
import asyncio
from typing import Optional

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
base_prompt = """
ë‹¹ì‹ ì€ ë¬¸ì¥ ë³€í™˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì¥ì„ ì§€ì •ëœ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ë³€í™˜ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ì½”ë“œ ë¸”ë¡ì´ë‚˜ ë”°ì˜´í‘œ ì—†ì´ ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

# ìŠ¤íƒ€ì¼ë³„ ì§€ì¹¨ ì •ì˜
style_instructions = {
    'formal': "ê²©ì‹ìˆê³  ê³µì‹ì ì¸ ì–´íˆ¬('-ìŠµë‹ˆë‹¤', '-ë‹ˆë‹¤'ì²´)ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
    'casual': "ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ì–´íˆ¬('-ì•¼', '-ì–´'ì²´)ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
    'polite': "ë§¤ìš° ê³µì†í•˜ê³  ì˜ˆì˜ë°”ë¥¸ ì–´íˆ¬('-ìš”'ì²´)ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
    'cute': "ê·€ì—½ê³  ì• êµìˆëŠ” ì–´íˆ¬('~ìš”'ì²´)ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”."
}

class PolyglotKoHandler:  # í´ë˜ìŠ¤ ì´ë¦„ ë³€ê²½
    def __init__(self):
        print("=== Polyglot-KO ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ ===")  # ë¡œê·¸ ë©”ì‹œì§€ë„ ìˆ˜ì •
        self.model_name = "EleutherAI/polyglot-ko-5.8b"
        self.model_loaded = False
        self.inference_timeout = 300
        self.emojis = ['ğŸ’•', 'âœ¨', 'ğŸ¥º', 'ğŸ˜Š', 'ğŸ’', 'ğŸŒ¸', 'ğŸ’—', 'ğŸ’–']
        
        try:
            print("1. í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            print("âœ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
            
            print("2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
            
            print("3. ëª¨ë¸ ë¡œë”© ì¤‘... (1-3ë¶„ ì†Œìš”)")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            self.model.eval()
            
            self.model_loaded = True
            print("=== ì´ˆê¸°í™” ì™„ë£Œ! ì„œë¹„ìŠ¤ ì¤€ë¹„ë¨ ===")
            
        except Exception as e:
            print(f"[ì—ëŸ¬] ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.model_loaded = False

    async def get_completion(self, message: str, style: str) -> Optional[str]:
        if not self.model_loaded:
            print("[ìƒíƒœ] ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
            
        try:
            # ìŠ¤íƒ€ì¼ì— í•´ë‹¹í•˜ëŠ” ì§€ì¹¨ ê°€ì ¸ì˜¤ê¸°
            style_instruction = style_instructions.get(style, "")
            if not style_instruction:
                print(f"[ê²½ê³ ] ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤íƒ€ì¼: {style}")
                return None

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""{base_prompt}
{style_instruction}

ì…ë ¥: "{message}"
"""

            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)

            # token_type_ids ì œê±°
            if 'token_type_ids' in inputs:
                del inputs['token_type_ids']

            print("[ì²˜ë¦¬] ì¶”ë¡  ì‹œì‘...")
            outputs = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **inputs,
                        max_new_tokens=100,
                        temperature=0.3,
                        do_sample=True,
                        top_p=0.85,
                        repetition_penalty=1.1,
                        num_beams=3,
                        early_stopping=True
                    )
                ),
                timeout=self.inference_timeout
            )
            print("[ì²˜ë¦¬] ì¶”ë¡  ì™„ë£Œ")

            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()

            # cute ìŠ¤íƒ€ì¼ì¼ ë•Œ ì´ëª¨ì§€ ì¶”ê°€
            if style == 'cute':
                emoji_count = random.randint(1, 2)
                selected_emojis = ' ' + ''.join(random.sample(self.emojis, emoji_count))
                response = response + selected_emojis

            return response

        except asyncio.TimeoutError:
            print(f"[íƒ€ì„ì•„ì›ƒ] {self.inference_timeout}ì´ˆ ì´ˆê³¼")
            return None
        except Exception as e:
            print(f"[ì—ëŸ¬] HuggingFace ëª¨ë¸ ì˜¤ë¥˜: {e}")
            return None