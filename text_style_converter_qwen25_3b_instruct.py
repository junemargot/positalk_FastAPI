from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from typing import Dict

model = None
tokenizer = None
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

base_prompt = """
ë‹¹ì‹ ì€ ë¬¸ì¥ ë³€í™˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì¥ì„ ì§€ì •ëœ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ë³€í™˜ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ì›ë¬¸ì˜ ì˜ë¯¸ëŠ” ë˜ë„ë¡ ìœ ì§€ë  ìˆ˜ ìˆë„ë¡ í•´ì£¼ì„¸ìš”.
"""

# ìŠ¤íƒ€ì¼ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì§€ì‹œì‚¬í•­
style_settings: Dict[str, Dict[str, str]] = {
    'formal': {
        'persona': """ë‹¹ì‹ ì€ ê²©ì‹ê³¼ ì˜ˆì˜ë¥¼ ì¤‘ì‹œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë¹„ê²©ì‹ì ì¸ í‘œí˜„ì„ ê²©ì‹ìˆê³  ê³µì‹ì ì¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
í•­ìƒ '-ìŠµë‹ˆë‹¤', '-ì…ë‹ˆë‹¤'ì™€ ê°™ì€ ê²©ì‹ì²´ë¥¼ ì‚¬ìš©í•˜ë©°, ì „ë¬¸ì ì´ê³  ê³µì‹ì ì¸ ì–´íœ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.""",
        'instruction': "ë‹¤ìŒ ë¬¸ì¥ì„ ê²©ì‹ìˆê³  ê³µì‹ì ì¸ ì–´íˆ¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
        'examples': [
            {"input": "ì´ê±° ë‚´ì¼ê¹Œì§€ í•´ì•¼ë¼", 
             "output": "í•´ë‹¹ ì—…ë¬´ë¥¼ ë‚´ì¼ê¹Œì§€ ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤."},
            {"input": "ê·¼ë° ì´ê²Œ ë§ë‚˜?", 
             "output": "ê·¸ëŸ¬ë‚˜ ì´ê²ƒì´ ì ì ˆí•œì§€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."}
        ]
    },
    'casual': {
        'persona': """ë‹¹ì‹ ì€ í¸ì•ˆí•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¥¼ êµ¬ì‚¬í•˜ëŠ” 20ëŒ€ì…ë‹ˆë‹¤.
ì–´ë–¤ ë¬¸ì¥ì´ë“  ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ ì¼ìƒ ëŒ€í™”ì²´ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'-ì•¼', '-ì–´', '-ì§€' ë“±ì˜ ë°˜ë§ì„ ì‚¬ìš©í•˜ê³ , êµ¬ì–´ì²´ í‘œí˜„ì„ ì ì ˆíˆ í™œìš©í•©ë‹ˆë‹¤.""",
        'instruction': "ë‹¤ìŒ ë¬¸ì¥ì„ ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ì–´íˆ¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
        'examples': [
            {"input": "íšŒì˜ ìë£Œë¥¼ ê²€í† í•˜ì—¬ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤", 
             "output": "íšŒì˜ ìë£Œ í•œë²ˆ ë´ì¤˜"},
            {"input": "ê¸ˆì¼ ì—…ë¬´ë³´ê³ ë¥¼ ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤", 
             "output": "ì˜¤ëŠ˜ ì—…ë¬´ë³´ê³  í• ê²Œ"}
        ]
    },
    'polite': {
        'persona': """ë‹¹ì‹ ì€ ì˜ˆì˜ ë°”ë¥¸ ë§íˆ¬ë¥¼ êµ¬ì‚¬í•˜ëŠ” ì„œë¹„ìŠ¤ì—… ì¢…ì‚¬ìì…ë‹ˆë‹¤.
í•­ìƒ ìƒëŒ€ë°©ì„ ì¡´ì¤‘í•˜ê³  ê³µì†í•œ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
'-ìš”', '-ì„¸ìš”'ì™€ ê°™ì€ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë©°, ì •ì¤‘í•˜ê³  ì¹œì ˆí•œ ì–´íœ˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.""",
        'instruction': "ë‹¤ìŒ ë¬¸ì¥ì„ ë§¤ìš° ê³µì†í•˜ê³  ì˜ˆì˜ë°”ë¥¸ ì–´íˆ¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
        'examples': [
            {"input": "ì´ê±° ì¢€ ë´ì¤˜", 
             "output": "ì´ê²ƒ ì¢€ ë´ì£¼ì‹œê² ì–´ìš”?"},
            {"input": "ì—¬ê¸°ì„œ ê¸°ë‹¤ë ¤", 
             "output": "ì´ê³³ì—ì„œ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œê² ì–´ìš”?"}
        ]
    },
    'cute': {
        'persona': """ë‹¹ì‹ ì€ ê·€ì—½ê³  ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¥¼ êµ¬ì‚¬í•˜ëŠ” ì•„ì´ëŒì…ë‹ˆë‹¤.
ì–´ë–¤ ë¬¸ì¥ì´ë“  ê·€ì—½ê³  ì• êµìˆëŠ” í‘œí˜„ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
'~ìš”', '~ì• ìš”', '~ëƒ¥' ë“±ì˜ ê·€ì—¬ìš´ ì–´ë¯¸ë¥¼ ì‚¬ìš©í•˜ê³ , ì´ëª¨í‹°ì½˜ì„ ì ì ˆíˆ í™œìš©í•©ë‹ˆë‹¤.""",
        'instruction': "ë‹¤ìŒ ë¬¸ì¥ì„ ê·€ì—½ê³  ì• êµìˆëŠ” ì–´íˆ¬ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.",
        'examples': [
            {"input": "ì•ˆë…•í•˜ì„¸ìš”", 
             "output": "ì•ˆë…•í•˜ì„¸ìš”~! â¤ï¸"},
            {"input": "ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”", 
             "output": "ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì‹¤ë˜ìš©~? ğŸ˜Š"}
        ]
    }
}

def init_pipeline(model_path: str = "Qwen/Qwen2.5-3B-Instruct") -> None:
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
    global model, tokenizer
    
    if device.type == "cuda":
        torch.backends.cudnn.benchmark = True
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
        device_map="auto",
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)

def create_style_prompt(text: str, style: str) -> str:
    """ìŠ¤íƒ€ì¼ ë³€í™˜ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    if style not in style_settings:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤: {style}")
        
    setting = style_settings[style]
    system_prompt = f"""
{setting['persona']}

ë‹¹ì‹ ì€ ë¬¸ì¥ ë³€í™˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì¥ì„ ì§€ì •ëœ ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ë³€í™˜ëœ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ì›ë¬¸ì˜ ì˜ë¯¸ë¥¼ í•´ì¹˜ì§€ ì•Šê³  ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ì¼ê´€ëœ ì–´íˆ¬ë¥¼ ìœ ì§€í•˜ê³ , ë¬¸ë§¥ì— ë§ëŠ” ì ì ˆí•œ ì–´íœ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.
"""

    # Few-shot ì˜ˆì‹œ ì¶”ê°€
    examples = "\n\nì˜ˆì‹œ:\n"
    for example in setting['examples']:
        examples += f"ì…ë ¥: {example['input']}\nì¶œë ¥: {example['output']}\n"

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{setting['instruction']}\n{examples}\n\në¬¸ì¥: {text}"}
    ]
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

@torch.inference_mode()
def generate_response(text: str, max_new_tokens: int = 512) -> str:
    """ì‘ë‹µ ìƒì„± - íŒŒë¼ë¯¸í„° ìµœì í™”"""
    inputs = tokenizer(
        [text],
        return_tensors="pt",
        padding=True,
        return_attention_mask=True
    ).to(device)
    
    outputs = model.generate(
        inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.6,  # ë” ì•ˆì •ì ì¸ ì¶œë ¥ì„ ìœ„í•´ ë‚®ì¶¤
        top_p=0.85,      # ë” ì§‘ì¤‘ëœ í™•ë¥  ë¶„í¬
        top_k=40,        # top-k sampling ì¶”ê°€
        repetition_penalty=1.3,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
        no_repeat_ngram_size=3,  # n-gram ë°˜ë³µ ë°©ì§€
        num_beams=3      # beam search ì ìš©
    )
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids 
        in zip(inputs.input_ids, outputs)
    ]
    
    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

def convert_style(text: str, style: str) -> str:
    """ìŠ¤íƒ€ì¼ ë³€í™˜ ì‹¤í–‰"""
    if model is None:
        init_pipeline()
        
    prompt = create_style_prompt(text, style)
    return generate_response(prompt) 

if __name__ == "__main__":
    init_pipeline()
    
    # í…ŒìŠ¤íŠ¸
    test_text = """2025ë…„ì€ AIê°€ í­ë°œì ìœ¼ë¡œ ì„±ì¥í•˜ëŠ” í•œ í•´ê°€ ë ê±°ì•¼.
ì™œëƒí•˜ë©´ AUNTê°€ ë³¸ê²©ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•œ í•´ë‹ˆê¹Œ."""
    
    print("ì›ë¬¸:", test_text)
    print("\nê° ìŠ¤íƒ€ì¼ë³„ ë³€í™˜ ê²°ê³¼:")
    
    for style in style_settings.keys():
        print(f"\n[{style} ìŠ¤íƒ€ì¼]")
        print(convert_style(test_text, style))
        print("-" * 50)